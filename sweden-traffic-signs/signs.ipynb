{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Описание\n",
    "\n",
    "Здесь всё то же самое, что с Симпсонами, но с аугментацией данных случайными поворотами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.io import imread\n",
    "from skimage.measure import block_reduce\n",
    "\n",
    "from scipy import ndimage\n",
    "from scipy.ndimage.filters import median_filter\n",
    "\n",
    "from keras import Model\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Flatten, Dense\n",
    "from keras.layers import Multiply\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization as BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "os.chdir(\"/content/drive/My Drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 32\n",
    "input_gray = False\n",
    "n_input_channels = 1 if input_gray else 3\n",
    "input_shape = (input_size,) * 2 + (n_input_channels,)\n",
    "dataset_root = Path(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(img_path):\n",
    "    img = imread(str(img_path), as_gray=input_gray)\n",
    "    img = img.reshape(img.shape[:-1] + (n_input_channels,))\n",
    "    img = img.astype(np.float32)\n",
    "\n",
    "    if not input_gray:\n",
    "        img /= 255\n",
    "\n",
    "    max_side = max(img.shape)\n",
    "\n",
    "    if max_side > input_size:\n",
    "        block_size = (max_side + input_size - 1) // input_size\n",
    "        img = block_reduce(img, block_size=(block_size,) * 2 + (1,), func=np.median)\n",
    "\n",
    "    dy = (input_size - img.shape[0]) // 2\n",
    "    dx = (input_size - img.shape[1]) // 2\n",
    "    img_sized = np.zeros(input_shape, dtype=np.float32)\n",
    "    img_sized[dy:img.shape[0]+dy, dx:img.shape[1]+dx] = img\n",
    "    img = img_sized\n",
    "    \n",
    "    img = median_filter(img, 3)\n",
    "    \n",
    "    m = img.min()\n",
    "    M = img.max()\n",
    "    img = (img - m) / (M - m)\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "def join_paths(*args):\n",
    "    return Path.joinpath(*(Path(arg) for arg in args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df[\"label\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "labels = dataset_df[\"label\"].unique()\n",
    "\n",
    "idx_by_label = {\n",
    "    label: i\n",
    "    for i, label in enumerate(labels)\n",
    "}\n",
    "\n",
    "label_min = 500\n",
    "\n",
    "erase_size_min = 5\n",
    "erase_size_max = 8\n",
    "\n",
    "dataset_x = []\n",
    "dataset_idx = []\n",
    "dataset_y = []\n",
    "\n",
    "count_by_label_idx = [0] * len(labels)\n",
    "\n",
    "for i, row in dataset_df.iterrows():\n",
    "    dataset_x.append(load_img(join_paths(dataset_root, \"data\", row[\"file_name\"])))\n",
    "    \n",
    "    label_idx = idx_by_label[row[\"label\"]]\n",
    "    dataset_idx.append(label_idx)\n",
    "    dataset_y.append(to_categorical(label_idx, num_classes=len(labels)))\n",
    "    count_by_label_idx[label_idx] += 1\n",
    "    \n",
    "for i in range(len(dataset_x)):\n",
    "    n = count_by_label_idx[dataset_idx[i]]\n",
    "    \n",
    "    if n >= label_min:\n",
    "        continue\n",
    "    \n",
    "    mul_factor = (label_min + n - 1) // n\n",
    "    \n",
    "    for j in range(mul_factor - 1):\n",
    "        img = dataset_x[i]\n",
    "        \n",
    "        angle = randint(-10, 11)\n",
    "        \n",
    "        img = ndimage.rotate(img, angle, reshape=False)\n",
    "        \n",
    "        m = img.min()\n",
    "        M = img.max()\n",
    "        img = (img - m) / (M - m)\n",
    "        \n",
    "#         erase_x1 = randint(input_size - erase_size_min)\n",
    "#         erase_y1 = randint(input_size - erase_size_min)\n",
    "\n",
    "#         erase_x2 = randint(\n",
    "#             erase_x1 + erase_size_min,\n",
    "#             min(input_size, erase_x1 + erase_size_max) + 1\n",
    "#         )\n",
    "        \n",
    "#         erase_y2 = randint(\n",
    "#             erase_y1 + erase_size_min,\n",
    "#             min(input_size, erase_y1 + erase_size_max) + 1\n",
    "#         )\n",
    "        \n",
    "#         img[erase_y1:erase_y2, erase_x1:erase_x2] = \\\n",
    "#             np.random.rand(erase_y2 - erase_y1, erase_x2 - erase_x1, n_input_channels)\n",
    "        \n",
    "        dataset_x.append(img)\n",
    "        dataset_y.append(dataset_y[i])\n",
    "\n",
    "dataset_x = np.array(dataset_x)\n",
    "dataset_y = np.array(dataset_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dataset_x[-])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_array(name):\n",
    "    np.save(f\"{name}.npy\", globals()[name])\n",
    "\n",
    "\n",
    "def save_arrays(*args):\n",
    "    for arg in args:\n",
    "        save_array(arg)\n",
    "\n",
    "        \n",
    "def load_array(name):\n",
    "    globals()[name] = np.load(f\"{name}.npy\")\n",
    "\n",
    "    \n",
    "def load_arrays(*args):\n",
    "    for arg in args:\n",
    "        load_array(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_arrays(\"dataset_x\", \"dataset_y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_arrays(\"dataset_x\", \"dataset_y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(dataset_x, dataset_y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_arrays(\"train_x\", \"test_x\", \"train_y\", \"test_y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_arrays(\"train_x\", \"test_x\", \"train_y\", \"test_y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"labels.txt\", \"w\") as f:\n",
    "    f.write(\" \".join(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"labels.txt\", \"r\") as f:\n",
    "    labels = f.read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n_epochs = 50\n",
    "\n",
    "use_global_avg_pooling = True\n",
    "use_se_block = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = \"elu\"\n",
    "\n",
    "model_input = Input(shape=input_shape)\n",
    "\n",
    "def block(x, k):\n",
    "    x = Conv2D(k, 3, padding=\"same\", activation=act)(x)\n",
    "    x = Conv2D(k, 3, activation=act)(x)\n",
    "    x = MaxPooling2D(2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = BatchNorm()(x)\n",
    "    return x\n",
    "\n",
    "def se_block(x, n):\n",
    "    branch = x\n",
    "    branch = GlobalAveragePooling2D()(branch)\n",
    "    branch = Dense(8, activation=act)(branch)\n",
    "    branch = Dense(n, activation=\"sigmoid\")(branch)\n",
    "    \n",
    "    return Multiply()([x, branch])\n",
    "\n",
    "model = model_input\n",
    "model = block(model, 32)\n",
    "model = block(model, 64)\n",
    "\n",
    "if use_se_block:\n",
    "    model = se_block(model, 64)\n",
    "\n",
    "model = block(model, 64)\n",
    "\n",
    "if use_global_avg_pooling:\n",
    "    model = GlobalAveragePooling2D()(model)\n",
    "else:\n",
    "    model = Flatten()(model)\n",
    "    model = Dense(512, activation=act)(model)\n",
    "    model = Dropout(0.5)(model)\n",
    "\n",
    "model = Dense(len(labels), activation=\"softmax\")(model)\n",
    "model = Model(inputs=model_input, outputs=model)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"categorical_accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp_save = ModelCheckpoint(\n",
    "    \"model.hdf5\", \n",
    "    verbose=1,\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_x, train_y,\n",
    "    validation_data=(test_x, test_y),\n",
    "    epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[mcp_save]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"model.hdf5\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "task_x = np.zeros((len(task_df),) + input_shape)\n",
    "\n",
    "for i, row in task_df.iterrows():\n",
    "    task_x[i] = load_img(join_paths(dataset_root, \"data\", row[\"file_name\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_y = model.predict(task_x, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_df = pd.DataFrame()\n",
    "\n",
    "ans_df[\"label\"] = [labels[v.argmax()] for v in task_y]\n",
    "\n",
    "ans_df.index = task_df[\"file_name\"]\n",
    "ans_df.index.name = \"file_name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 200\n",
    "print(ans_df.iloc[i][\"label\"])\n",
    "plt.imshow(task_x[i])\n",
    "# pd.read_csv(\"train.csv\")[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_df.to_csv(\"ans.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
