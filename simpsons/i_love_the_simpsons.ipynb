{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.measure import block_reduce\n",
    "from scipy import ndimage\n",
    "\n",
    "from keras import Model\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Flatten, Dense\n",
    "from keras.layers import Multiply\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization as BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "os.chdir(\"/content/drive/My Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обработка данных\n",
    "\n",
    "Все картинки приводятся к размеру $64 \\times 64 \\times 3$ взятием медианы на блоке, затем применяется min-max scaling.\n",
    "\n",
    "Возможна аугментация данных (представителей некоторых классов очень мало), в частности, случайные повороты и случайные вырезы, однако лучший результат на Kaggle был получен без неё."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 64\n",
    "input_shape = (input_size,) * 2 + (3,)\n",
    "dataset_root = Path(\"simpsons_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(img_path):\n",
    "    img = imread(str(img_path)).astype(np.float32) / 255\n",
    "\n",
    "    max_side = max(img.shape)\n",
    "\n",
    "    if max_side > input_size:\n",
    "        block_size = (max_side + input_size - 1) // input_size\n",
    "        img = block_reduce(img, block_size=(block_size,) * 2 + (1,), func=np.median)\n",
    "\n",
    "    dy = (input_size - img.shape[0]) // 2\n",
    "    dx = (input_size - img.shape[1]) // 2\n",
    "    img_sized = np.zeros(input_shape, dtype=np.float32)\n",
    "    img_sized[dy:img.shape[0]+dy, dx:img.shape[1]+dx] = img\n",
    "    img = img_sized\n",
    "    \n",
    "    m = img.min()\n",
    "    M = img.max()\n",
    "    img = (img - m) / (M - m)\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "def join_paths(*args):\n",
    "    return Path.joinpath(*(Path(arg) for arg in args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_data = False\n",
    "\n",
    "def dir_has_data(p):\n",
    "    if not p.is_dir():\n",
    "        return False\n",
    "    \n",
    "    for x in p.iterdir():\n",
    "        if x.suffix == \".jpg\":\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "characters = [p.name for p in dataset_root.iterdir() if dir_has_data(p)]\n",
    "\n",
    "character_min = 500\n",
    "\n",
    "erase_size_min = 5\n",
    "erase_size_max = 8\n",
    "\n",
    "dataset_x = []\n",
    "dataset_idx = []\n",
    "dataset_y = []\n",
    "\n",
    "count_by_character_idx = [0] * len(characters)\n",
    "\n",
    "print(\"Loading data...\")\n",
    "progress_bar = \".\" * len(characters)\n",
    "print(f\"[{progress_bar}] 0%\", end=\"\\r\")\n",
    "\n",
    "for character_idx, character in enumerate(characters):\n",
    "    character_dir = join_paths(dataset_root, character)\n",
    "    \n",
    "    character_vec = to_categorical(\n",
    "        character_idx,\n",
    "        num_classes=len(characters)\n",
    "    )\n",
    "    \n",
    "    for img_path in character_dir.iterdir():\n",
    "        if img_path.suffix != \".jpg\":\n",
    "            continue\n",
    "        \n",
    "        img = load_img(img_path)\n",
    "        \n",
    "        dataset_x.append(img)\n",
    "        dataset_idx.append(character_idx)\n",
    "        dataset_y.append(character_vec)\n",
    "        count_by_character_idx[character_idx] += 1\n",
    "    \n",
    "    done = \"=\" * (character_idx + 1)\n",
    "    left = \".\" * (len(characters) - character_idx - 1)\n",
    "    progress = (character_idx + 1) / len(characters) * 100\n",
    "    print(f\"[{done}{left}] {progress:.1f}% \", end=\"\\r\")\n",
    "    \n",
    "print()\n",
    "\n",
    "if augment_data:\n",
    "    print(\"Augmenting data...\")\n",
    "    progress_k = len(dataset_x) // len(characters)\n",
    "    progress_max = len(dataset_x) // progress_k\n",
    "    progress_bar = \".\" * progress_max\n",
    "    print(f\"[{progress_bar}] 0%\", end=\"\\r\")\n",
    "\n",
    "    for i in range(len(dataset_x)):\n",
    "        n = count_by_character_idx[dataset_idx[i]]\n",
    "\n",
    "        if n >= character_min:\n",
    "            continue\n",
    "\n",
    "        mul_factor = (character_min + n - 1) // n\n",
    "\n",
    "        for j in range(mul_factor - 1):\n",
    "            img = dataset_x[i]\n",
    "\n",
    "            angle = randint(-10, 11)\n",
    "\n",
    "            img = ndimage.rotate(img, angle, reshape=False)\n",
    "\n",
    "            m = img.min()\n",
    "            M = img.max()\n",
    "            img = (img - m) / (M - m)\n",
    "\n",
    "            erase_x1 = randint(input_size - erase_size_min)\n",
    "            erase_y1 = randint(input_size - erase_size_min)\n",
    "\n",
    "            erase_x2 = randint(\n",
    "                erase_x1 + erase_size_min,\n",
    "                min(input_size, erase_x1 + erase_size_max) + 1\n",
    "            )\n",
    "\n",
    "            erase_y2 = randint(\n",
    "                erase_y1 + erase_size_min,\n",
    "                min(input_size, erase_y1 + erase_size_max) + 1\n",
    "            )\n",
    "\n",
    "            img[erase_y1:erase_y2, erase_x1:erase_x2] = \\\n",
    "                np.random.rand(erase_y2 - erase_y1, erase_x2 - erase_x1, 3)\n",
    "\n",
    "            dataset_x.append(img)\n",
    "            dataset_y.append(dataset_y[i])\n",
    "\n",
    "        progress_idx = (i + 1) // progress_k\n",
    "        done = \"=\" * progress_idx\n",
    "        left = \".\" * (progress_max - progress_idx)\n",
    "        progress = progress_idx / progress_max * 100\n",
    "        print(f\"[{done}{left}] {progress:.1f}% \", end=\"\\r\")\n",
    "\n",
    "dataset_x = np.array(dataset_x)\n",
    "dataset_y = np.array(dataset_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_array(name):\n",
    "    np.save(f\"{name}.npy\", globals()[name])\n",
    "\n",
    "\n",
    "def save_arrays(*args):\n",
    "    for arg in args:\n",
    "        save_array(arg)\n",
    "\n",
    "        \n",
    "def load_array(name):\n",
    "    globals()[name] = np.load(f\"{name}.npy\")\n",
    "\n",
    "    \n",
    "def load_arrays(*args):\n",
    "    for arg in args:\n",
    "        load_array(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_arrays(\"dataset_x\", \"dataset_y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_arrays(\"dataset_x\", \"dataset_y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(dataset_x, dataset_y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_arrays(\"train_x\", \"test_x\", \"train_y\", \"test_y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_arrays(\"train_x\", \"test_x\", \"train_y\", \"test_y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"characters.txt\", \"w\") as f:\n",
    "    f.write(\" \".join(characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"characters.txt\", \"r\") as f:\n",
    "    characters = f.read().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Архитектура нейросети\n",
    "\n",
    "Пусть $B\\left(n\\right)$ – следующая последовательность слоёв: свёртка из $n$ фильров $3 \\times 3$, затем ещё раз такая же свёртка, потом max pooling $2 \\times 2$, dropout с вероятностью $0.25$ и batch-нормализация. После каждой свёртки используется активация $f\\left(x\\right) = x$ при $x \\gt 0$ и $f\\left(x\\right) = e^x - 1$ при $x \\le 0$, известная как exponential linear unit.\n",
    "\n",
    "Нейросеть начинается с последовательности блоков $B\\left(32\\right) \\mapsto B\\left(64\\right) \\mapsto B\\left(64\\right)$. Изначально после этого шёл полносвязный слой из $512$ нейронов с dropout с вероятностью $0.5$, однако его замена на global average pooling, то есть подсчёт на каждом канале среднего значения, привела к заметному улучшению точности и значительному упрощению нейросети (в конечном варианте меньше $150$ тысяч параметров). В самом конце стоит выходной слой с активацией softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n_epochs = 50\n",
    "\n",
    "use_global_avg_pooling = True\n",
    "use_se_block = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = \"elu\"\n",
    "\n",
    "model_input = Input(shape=input_shape)\n",
    "\n",
    "def block(x, k):\n",
    "    x = Conv2D(k, 3, padding=\"same\", activation=act)(x)\n",
    "    x = Conv2D(k, 3, activation=act)(x)\n",
    "    x = MaxPooling2D(2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = BatchNorm()(x)\n",
    "    return x\n",
    "\n",
    "def se_block(x, n):\n",
    "    branch = x\n",
    "    branch = GlobalAveragePooling2D()(branch)\n",
    "    branch = Dense(8, activation=act)(branch)\n",
    "    branch = Dense(n, activation=\"sigmoid\")(branch)\n",
    "    \n",
    "    return Multiply()([x, branch])\n",
    "\n",
    "model = model_input\n",
    "model = block(model, 32)\n",
    "model = block(model, 64)\n",
    "\n",
    "if use_se_block:\n",
    "    model = se_block(model, 64)\n",
    "\n",
    "model = block(model, 64)\n",
    "\n",
    "if use_global_avg_pooling:\n",
    "    model = GlobalAveragePooling2D()(model)\n",
    "else:\n",
    "    model = Flatten()(model)\n",
    "    model = Dense(512, activation=act)(model)\n",
    "    model = Dropout(0.5)(model)\n",
    "\n",
    "model = Dense(len(characters), activation=\"softmax\")(model)\n",
    "model = Model(inputs=model_input, outputs=model)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"categorical_accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp_save = ModelCheckpoint(\n",
    "    \"model.hdf5\", \n",
    "    verbose=1,\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_x, train_y,\n",
    "    validation_data=(test_x, test_y),\n",
    "    epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[mcp_save]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"model.hdf5\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(test_x, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построение матрицы ошибок\n",
    "\n",
    "На матрице видно, например, что нейросеть часто принимает Пэтти за Сельму, что предсказуемо, ведь их головы отличаются только причёсками, которые на некоторых кадрах трудно отличить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = np.zeros((len(characters),) * 2, dtype=np.float32)\n",
    "\n",
    "accuracy = [[0, 0] for _ in range(len(characters))]\n",
    "\n",
    "for i in range(test_pred.shape[0]):\n",
    "    predicted = test_pred[i].argmax()\n",
    "    expected = test_y[i].argmax()\n",
    "    \n",
    "    accuracy[expected][1] += 1\n",
    "    \n",
    "    if predicted == expected:\n",
    "        accuracy[expected][0] += 1\n",
    "    else:\n",
    "        confusion[predicted, expected] += 1\n",
    "\n",
    "accs = []\n",
    "        \n",
    "for i, (n_correct, n_total) in enumerate(accuracy):\n",
    "    if n_total == 0:\n",
    "        accs.append((i, 0, n_total))\n",
    "    else:\n",
    "        acc = n_correct / n_total * 100\n",
    "        accs.append((i, acc, n_total))\n",
    "        \n",
    "accs.sort(key = lambda x: x[1])\n",
    "        \n",
    "for i, acc, n_total in accs:\n",
    "    character = characters[i]\n",
    "    n_train = sum(1 for _ in join_paths(dataset_root, character).iterdir())\n",
    "    print(f\"{character}: {acc:.1f} ({n_total}, {n_train})\")\n",
    "    \n",
    "plt.figure(figsize=(20, 20))\n",
    "        \n",
    "for axis, deg in [(plt.xticks, 90), (plt.yticks, 0)]:\n",
    "    axis(list(range(len(characters))), characters, rotation=deg)\n",
    "\n",
    "confusion /= confusion.max()\n",
    "\n",
    "plt.imshow(confusion, origin=\"lower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_char = \"patty_bouvier\"\n",
    "pred_char = \"selma_bouvier\"\n",
    "\n",
    "# exp_char = \"miss_hoover\"\n",
    "# pred_char = \"martin_prince\"\n",
    "\n",
    "mispred = [\n",
    "    i for i, v in enumerate(test_y) \n",
    "    if \\\n",
    "        characters[v.argmax()] == exp_char and \\\n",
    "        characters[test_pred[i].argmax()] == pred_char\n",
    "]\n",
    "\n",
    "i = mispred[randint(len(mispred))]\n",
    "img = test_x[i]\n",
    "pred = list(enumerate(model.predict(np.array([img]))[0]))\n",
    "pred.sort(key = lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Expected:\", characters[test_y[i].argmax()])\n",
    "\n",
    "for i, p in pred[:5]:\n",
    "    print(characters[i], p)\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_root = Path(\"testset\")\n",
    "\n",
    "task_names = []\n",
    "task_x = []\n",
    "\n",
    "for img_path in task_root.iterdir():\n",
    "    if img_path.suffix != \".jpg\":\n",
    "        continue\n",
    "    \n",
    "    img = load_img(img_path)\n",
    "    \n",
    "    task_names.append(img_path.name)\n",
    "    task_x.append(img)\n",
    "\n",
    "task_x = np.array(task_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"task_names.txt\", \"w\") as f:\n",
    "    f.write(\" \".join(task_names))\n",
    "\n",
    "save_arrays(\"task_x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"task_names.txt\", \"r\") as f:\n",
    "    task_names = f.read().split()\n",
    "\n",
    "load_arrays(\"task_x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_y = model.predict(task_x, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_df = pd.DataFrame()\n",
    "\n",
    "ans_df[\"Expected\"] = [characters[v.argmax()] for v in task_y]\n",
    "\n",
    "ans_df.index = task_names\n",
    "ans_df.index.name = \"Id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_df[ans_df[\"Expected\"] == \"lionel_hutz\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_df.to_csv(\"ans.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
